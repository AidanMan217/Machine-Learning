{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d76ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559eda32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1   x2    t1    t2\n",
       "0  1.0  0.5  10.0   5.0\n",
       "1  2.5  1.3  20.5  10.2\n",
       "2  3.2  2.1  30.1  15.3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pd.read_csv('regr2.csv')\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c32907c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.5]\n",
      " [2.5 1.3]\n",
      " [3.2 2.1]]\n",
      "[[10.   5. ]\n",
      " [20.5 10.2]\n",
      " [30.1 15.3]]\n"
     ]
    }
   ],
   "source": [
    "X = db[['x1', 'x2']].values\n",
    "y = db[['t1','t2']].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3883ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass #\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x) #works componently with numpy arrays\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return np.where(x > 0, 1, 0) #works componently with numpy arrays (condition, if condition true then x, else y)\n",
    "    \n",
    "def forward(X,w1,w2,w3):\n",
    "    # X: inputs\n",
    "    # w1: weights input -> hidden1 adjusted for bias\n",
    "    # w2: weights hidden1 -> hidden2 adjusted for bias\n",
    "    # w3: weights hidden2 -> output adjusted for bias\n",
    "    Xc = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "    h1 = relu(Xc @ w1)\n",
    "    h1c = np.concatenate([h1, np.ones((h1.shape[0], 1))], axis=1)\n",
    "    h2 = relu(h1c @ w2)\n",
    "    h2c = np.concatenate([h2, np.ones((h2.shape[0], 1))], axis=1)\n",
    "    o1 = identity(h2c @ w3)\n",
    "    return o1, h1, h2, Xc, h1c, h2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e3e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass #\n",
    "\n",
    "def backward(o,t,w2,w3,h2,h1,h1c,h2c,Xc):\n",
    "    # o: output of forward pass\n",
    "    # t: target values\n",
    "    # w2: weights hidden1 -> hidden2 removed bias\n",
    "    # w3: weights hidden2 -> output removed bias\n",
    "    # h2: hidden2 activations\n",
    "    # h1: hidden1 activations\n",
    "    # h1c: hidden1 activations with bias\n",
    "    # h2c: hidden2 activations with bias\n",
    "    # Xc: inputs with bias\n",
    "\n",
    "    delta_3 = o - t\n",
    "    delta_2 = relu_deriv(h2)* (delta_3 @ w3[:-1].T)\n",
    "    delta_1 = relu_deriv(h1)* (delta_2 @ w2[:-1].T)\n",
    "\n",
    "    dW3 = h2c.T @ delta_3\n",
    "    dW2 = h1c.T @ delta_2\n",
    "    dW1 = Xc.T @ delta_1 \n",
    "    #I forgot the gradient part, and remember its included with bias, huge mistake\n",
    "\n",
    "    return dW3, dW2, dW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a668d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent #\n",
    "\n",
    "def sgd(W,dW,eta):\n",
    "    return [W_i - (eta * dW_i) for W_i, dW_i in zip(W, dW)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f5ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 257.0590768581208\n",
      "Epoch 100: Loss = 0.41430122009257225\n",
      "Epoch 200: Loss = 0.3491366172201107\n",
      "Epoch 300: Loss = 0.2933630529113312\n",
      "Epoch 400: Loss = 0.24472499691749425\n",
      "Epoch 500: Loss = 0.2026616839921851\n",
      "Epoch 600: Loss = 0.16673447693121898\n",
      "Epoch 700: Loss = 0.1364770272531392\n",
      "Epoch 800: Loss = 0.1113652039161202\n",
      "Epoch 900: Loss = 0.09081823025640569\n",
      "Epoch 1000: Loss = 0.0742196900690831\n",
      "Epoch 1100: Loss = 0.060949188799924205\n",
      "Epoch 1200: Loss = 0.05041523458304751\n",
      "Epoch 1300: Loss = 0.04208194385856639\n",
      "Epoch 1400: Loss = 0.03548565977994316\n",
      "Epoch 1500: Loss = 0.03024104025696785\n",
      "Epoch 1600: Loss = 0.02603857226007712\n",
      "Epoch 1700: Loss = 0.02263649107656482\n",
      "Epoch 1800: Loss = 0.01985000340315013\n",
      "Epoch 1900: Loss = 0.01754003475240444\n",
      "Epoch 2000: Loss = 0.01560288991575981\n",
      "Epoch 2100: Loss = 0.013961498505942643\n",
      "Epoch 2200: Loss = 0.012558416394409687\n",
      "Epoch 2300: Loss = 0.011350462421758722\n",
      "Epoch 2400: Loss = 0.010304736726957834\n",
      "Epoch 2500: Loss = 0.009395734478396785\n",
      "Epoch 2600: Loss = 0.008603289990036704\n",
      "Epoch 2700: Loss = 0.007911129863152908\n",
      "Epoch 2800: Loss = 0.007305861807172513\n",
      "Epoch 2900: Loss = 0.00677626927955195\n",
      "Epoch 3000: Loss = 0.006312817748643493\n",
      "Epoch 3100: Loss = 0.005907305895673343\n",
      "Epoch 3200: Loss = 0.005552615430631988\n",
      "Epoch 3300: Loss = 0.005242527815954191\n",
      "Epoch 3400: Loss = 0.004971586454103075\n",
      "Epoch 3500: Loss = 0.0047349899722886205\n",
      "Epoch 3600: Loss = 0.004528507048525077\n",
      "Epoch 3700: Loss = 0.004348406454489789\n",
      "Epoch 3800: Loss = 0.0041913981382754725\n",
      "Epoch 3900: Loss = 0.004054582584100732\n",
      "Epoch 4000: Loss = 0.003935406608908333\n",
      "Epoch 4100: Loss = 0.003831624353214177\n",
      "Epoch 4200: Loss = 0.003741262607505121\n",
      "Epoch 4300: Loss = 0.0036625898607240575\n",
      "Epoch 4400: Loss = 0.0035940886135087577\n",
      "Epoch 4500: Loss = 0.003534430598404275\n",
      "Epoch 4600: Loss = 0.0034824546133977344\n",
      "Epoch 4700: Loss = 0.003437146717435022\n",
      "Epoch 4800: Loss = 0.003397622565708224\n",
      "Epoch 4900: Loss = 0.003363111683817666\n",
      "Epoch 5000: Loss = 0.0033329434967386325\n",
      "Epoch 5100: Loss = 0.0033065349427778053\n",
      "Epoch 5200: Loss = 0.0032833795155295387\n",
      "Epoch 5300: Loss = 0.0032630375888176694\n",
      "Epoch 5400: Loss = 0.0032451278910231255\n",
      "Epoch 5500: Loss = 0.0032293200061680022\n",
      "Epoch 5600: Loss = 0.0032153277896559336\n",
      "Epoch 5700: Loss = 0.003202903596632584\n",
      "Epoch 5800: Loss = 0.003191833230493371\n",
      "Epoch 5900: Loss = 0.003181931528064694\n",
      "Epoch 6000: Loss = 0.0031730385064114657\n",
      "Epoch 6100: Loss = 0.003165016004039491\n",
      "Epoch 6200: Loss = 0.003157744756469665\n",
      "Epoch 6300: Loss = 0.003151121852764636\n",
      "Epoch 6400: Loss = 0.0031450585256035794\n",
      "Epoch 6500: Loss = 0.003139478232950778\n",
      "Epoch 6600: Loss = 0.003134314994278733\n",
      "Epoch 6700: Loss = 0.00312951194871952\n",
      "Epoch 6800: Loss = 0.0031250201064643907\n",
      "Epoch 6900: Loss = 0.0031207972682508554\n",
      "Epoch 7000: Loss = 0.0031168070908998927\n",
      "Epoch 7100: Loss = 0.0031130182796338963\n",
      "Epoch 7200: Loss = 0.003109403890352263\n",
      "Epoch 7300: Loss = 0.0031059407271961325\n",
      "Epoch 7400: Loss = 0.0031026088226258127\n",
      "Epoch 7500: Loss = 0.0030993909889039744\n",
      "Epoch 7600: Loss = 0.0030962724313262527\n",
      "Epoch 7700: Loss = 0.003093240414821055\n",
      "Epoch 7800: Loss = 0.0030902839766501545\n",
      "Epoch 7900: Loss = 0.003087393678910783\n",
      "Epoch 8000: Loss = 0.003084561395386222\n",
      "Epoch 8100: Loss = 0.0030817801280256657\n",
      "Epoch 8200: Loss = 0.003079043848972967\n",
      "Epoch 8300: Loss = 0.003076347364616353\n",
      "Epoch 8400: Loss = 0.003073686198615056\n",
      "Epoch 8500: Loss = 0.003071056491269731\n",
      "Epoch 8600: Loss = 0.0030684549129711158\n",
      "Epoch 8700: Loss = 0.003065878589766603\n",
      "Epoch 8800: Loss = 0.0030633250393580142\n",
      "Epoch 8900: Loss = 0.0030607921160756683\n",
      "Epoch 9000: Loss = 0.0030582779635764897\n",
      "Epoch 9100: Loss = 0.0016968722413193695\n",
      "Epoch 9200: Loss = 0.0016171407454882469\n",
      "Epoch 9300: Loss = 0.0015493484695909378\n",
      "Epoch 9400: Loss = 0.0014931976137947227\n",
      "Epoch 9500: Loss = 0.0014491017206096854\n",
      "Epoch 9600: Loss = 0.0014032439711078983\n",
      "Epoch 9700: Loss = 0.0013605167329514528\n",
      "Epoch 9800: Loss = 0.001325337459547872\n",
      "Epoch 9900: Loss = 0.0012949587913701275\n",
      "[[10.02558092  4.95108752]\n",
      " [20.46755888 10.25327483]\n",
      " [30.1110672  15.2780914 ]]\n"
     ]
    }
   ],
   "source": [
    "# now we do a loop, inside it is initialized weights, foward pass, backward, and sgd\n",
    "\n",
    "def train(X,y,eta=0.01,epochs=1000):\n",
    "    #initialize weights\n",
    "    w1 = np.random.randn(X.shape[1]+1, 3)\n",
    "    w2 = np.random.randn(3+1,3)\n",
    "    w3 = np.random.randn(3+1, 2) #+1 for bias\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        o, h1, h2, Xc, h1c, h2c = forward(X,w1,w2,w3) #output, h1, h2, Xc, h1c, h2c\n",
    "        dW3, dW2, dW1 = backward(o,y,w2,w3,h2,h1,h1c,h2c,Xc) #outputs gradient of weights\n",
    "        w1, w2, w3 = sgd([w1,w2,w3],[dW1,dW2,dW3],eta) #update weights using sgd\n",
    "        if epoch % 100 == 0:\n",
    "           print(f\"Epoch {epoch}: Loss = {np.mean(np.square(o - y))}\")\n",
    "    return o, w1, w2, w3 #weights need to be returned to be used in the future\n",
    "\n",
    "print(train(X,y,0.001,10000)[0]) #the code error is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
