{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d76ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "559eda32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1   x2    t1    t2\n",
       "0  1.0  0.5  10.0   5.0\n",
       "1  2.5  1.3  20.5  10.2\n",
       "2  3.2  2.1  30.1  15.3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pd.read_csv('regr2.csv')\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c32907c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.5]\n",
      " [2.5 1.3]\n",
      " [3.2 2.1]]\n",
      "[[10.   5. ]\n",
      " [20.5 10.2]\n",
      " [30.1 15.3]]\n"
     ]
    }
   ],
   "source": [
    "X = db[['x1', 'x2']].values\n",
    "y = db[['t1','t2']].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc3883ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass #\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x) #works componently with numpy arrays\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return np.where(x > 0, 1, 0) #works componently with numpy arrays (condition, if condition true then x, else y)\n",
    "    \n",
    "def forward(X,w1,w2,w3):\n",
    "    # X: inputs\n",
    "    # w1: weights input -> hidden1 adjusted for bias\n",
    "    # w2: weights hidden1 -> hidden2 adjusted for bias\n",
    "    # w3: weights hidden2 -> output adjusted for bias\n",
    "    Xc = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "    h1 = relu(Xc @ w1)\n",
    "    h1c = np.concatenate([h1, np.ones((h1.shape[0], 1))], axis=1)\n",
    "    h2 = relu(h1c @ w2)\n",
    "    h2c = np.concatenate([h2, np.ones((h2.shape[0], 1))], axis=1)\n",
    "    o1 = identity(h2c @ w3)\n",
    "    return o1, h1, h2, Xc, h1c, h2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1e3e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass #\n",
    "\n",
    "def backward(o,t,w2,w3,h2,h1,h1c,h2c,Xc):\n",
    "    # o: output of forward pass\n",
    "    # t: target values\n",
    "    # w2: weights hidden1 -> hidden2 removed bias\n",
    "    # w3: weights hidden2 -> output removed bias\n",
    "    # h2: hidden2 activations\n",
    "    # h1: hidden1 activations\n",
    "    # h1c: hidden1 activations with bias\n",
    "    # h2c: hidden2 activations with bias\n",
    "    # Xc: inputs with bias\n",
    "\n",
    "    delta_3 = o - t\n",
    "    delta_2 = relu_deriv(h2)* (delta_3 @ w3[:-1].T)\n",
    "    delta_1 = relu_deriv(h1)* (delta_2 @ w2[:-1].T)\n",
    "\n",
    "    dW3 = h2c.T @ delta_3\n",
    "    dW2 = h1c.T @ delta_2\n",
    "    dW1 = Xc.T @ delta_1 \n",
    "    #I forgot the gradient part, and remember its included with bias, huge mistake\n",
    "\n",
    "    return dW3, dW2, dW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a668d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent #\n",
    "\n",
    "def sgd(W,dW,eta):\n",
    "    new_weights = []\n",
    "    for W_i, dW_i in zip(W, dW): #since sgd is in the training loop, we can use zip to iterate over the weights and gradients together and not need to loop inside the function\n",
    "        updated = W_i - eta * dW_i\n",
    "        new_weights.append(updated)\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5f5ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 360.69270673807233\n",
      "Epoch 100: Loss = 290.6918476376073\n",
      "Epoch 200: Loss = 258.44002502888276\n",
      "Epoch 300: Loss = 202.4339198225507\n",
      "Epoch 400: Loss = 122.84386228893118\n",
      "Epoch 500: Loss = 61.597086596078206\n",
      "Epoch 600: Loss = 32.74573332538816\n",
      "Epoch 700: Loss = 16.56555871805452\n",
      "Epoch 800: Loss = 7.282216297912227\n",
      "Epoch 900: Loss = 2.8613201329161733\n",
      "Epoch 1000: Loss = 1.0415552797295196\n",
      "Epoch 1100: Loss = 0.3677953903351476\n",
      "Epoch 1200: Loss = 0.1419359383621012\n",
      "Epoch 1300: Loss = 0.06802079462899216\n",
      "Epoch 1400: Loss = 0.04339864986281714\n",
      "Epoch 1500: Loss = 0.033858936561193605\n",
      "Epoch 1600: Loss = 0.02844876314360434\n",
      "Epoch 1700: Loss = 0.025504709449908586\n",
      "Epoch 1800: Loss = 0.023321203477082404\n",
      "Epoch 1900: Loss = 0.021444250444790886\n",
      "Epoch 2000: Loss = 0.019740677171160497\n",
      "Epoch 2100: Loss = 0.018182126401242737\n",
      "Epoch 2200: Loss = 0.01675188320980853\n",
      "Epoch 2300: Loss = 0.015437400260410655\n",
      "Epoch 2400: Loss = 0.014228299942701047\n",
      "Epoch 2500: Loss = 0.01311556937328285\n",
      "Epoch 2600: Loss = 0.012091189527421164\n",
      "Epoch 2700: Loss = 0.011147935712548027\n",
      "Epoch 2800: Loss = 0.010279252189870006\n",
      "Epoch 2900: Loss = 0.009479163239949502\n",
      "Epoch 3000: Loss = 0.008742204864535877\n",
      "Epoch 3100: Loss = 0.008063369710075895\n",
      "Epoch 3200: Loss = 0.007438061247179981\n",
      "Epoch 3300: Loss = 0.006862054812204294\n",
      "Epoch 3400: Loss = 0.00633146392994493\n",
      "Epoch 3500: Loss = 0.005842710810901187\n",
      "Epoch 3600: Loss = 0.005392500220563728\n",
      "Epoch 3700: Loss = 0.004977796124903293\n",
      "Epoch 3800: Loss = 0.004595800661858828\n",
      "Epoch 3900: Loss = 0.004243935093234522\n",
      "Epoch 4000: Loss = 0.0039198224675741035\n",
      "Epoch 4100: Loss = 0.0036212717804770967\n",
      "Epoch 4200: Loss = 0.003346263460261557\n",
      "Epoch 4300: Loss = 0.0030929360378276305\n",
      "Epoch 4400: Loss = 0.002859573882969981\n",
      "Epoch 4500: Loss = 0.0026445959072584567\n",
      "Epoch 4600: Loss = 0.0024465451474553856\n",
      "Epoch 4700: Loss = 0.0022640791543394965\n",
      "Epoch 4800: Loss = 0.0020959611205286235\n",
      "Epoch 4900: Loss = 0.0019410516879948454\n",
      "Epoch 5000: Loss = 0.0017983013818560236\n",
      "Epoch 5100: Loss = 0.001666743621996871\n",
      "Epoch 5200: Loss = 0.001545488268338235\n",
      "Epoch 5300: Loss = 0.0014337156592772332\n",
      "Epoch 5400: Loss = 0.001330671106103718\n",
      "Epoch 5500: Loss = 0.001235659809117218\n",
      "Epoch 5600: Loss = 0.001148042163797044\n",
      "Epoch 5700: Loss = 0.0010672294277752682\n",
      "Epoch 5800: Loss = 0.000992679721538798\n",
      "Epoch 5900: Loss = 0.0009238943377888336\n",
      "Epoch 6000: Loss = 0.0008604143362366244\n",
      "Epoch 6100: Loss = 0.0008018174023098297\n",
      "Epoch 6200: Loss = 0.0007477149498287431\n",
      "Epoch 6300: Loss = 0.0006977494491675315\n",
      "Epoch 6400: Loss = 0.0006515919637725917\n",
      "Epoch 6500: Loss = 0.0006089398791731172\n",
      "Epoch 6600: Loss = 0.0005695148097816133\n",
      "Epoch 6700: Loss = 0.0005330606698752611\n",
      "Epoch 6800: Loss = 0.0004993418961527664\n",
      "Epoch 6900: Loss = 0.00046814181020245286\n",
      "Epoch 7000: Loss = 0.000439261110084369\n",
      "Epoch 7100: Loss = 0.00041251648104318335\n",
      "Epoch 7200: Loss = 0.0003877393161114807\n",
      "Epoch 7300: Loss = 0.0003647745380668958\n",
      "Epoch 7400: Loss = 0.00034347951484668655\n",
      "Epoch 7500: Loss = 0.0003237230611271667\n",
      "Epoch 7600: Loss = 0.000305384519326169\n",
      "Epoch 7700: Loss = 0.00028835291380581153\n",
      "Epoch 7800: Loss = 0.000272526172525818\n",
      "Epoch 7900: Loss = 0.00025781041084230915\n",
      "Epoch 8000: Loss = 0.00024411927255310132\n",
      "Epoch 8100: Loss = 0.00023137332367049416\n",
      "Epoch 8200: Loss = 0.00021949949475243838\n",
      "Epoch 8300: Loss = 0.00020843056794475918\n",
      "Epoch 8400: Loss = 0.00019810470518933902\n",
      "Epoch 8500: Loss = 0.00018846501432670613\n",
      "Epoch 8600: Loss = 0.00017945915007964674\n",
      "Epoch 8700: Loss = 0.00017103894713868658\n",
      "Epoch 8800: Loss = 0.0001631600827889896\n",
      "Epoch 8900: Loss = 0.00015578176672008599\n",
      "Epoch 9000: Loss = 0.0001488664558446747\n",
      "Epoch 9100: Loss = 0.000142379592125115\n",
      "Epoch 9200: Loss = 0.00013628936156378986\n",
      "Epoch 9300: Loss = 0.0001305664726600379\n",
      "Epoch 9400: Loss = 0.00012518395277054972\n",
      "Epoch 9500: Loss = 0.00012011696093469099\n",
      "Epoch 9600: Loss = 0.00011534261584002854\n",
      "Epoch 9700: Loss = 0.00011083983770967859\n",
      "Epoch 9800: Loss = 0.00010658920298908125\n",
      "Epoch 9900: Loss = 0.00010257281080137361\n",
      "[[ 9.99521486  5.00470001]\n",
      " [20.51334362 10.18510161]\n",
      " [30.09136097 15.30855757]]\n"
     ]
    }
   ],
   "source": [
    "# now we do a loop, inside it is initialized weights, foward pass, backward, and sgd\n",
    "\n",
    "def train(X,y,eta=0.01,epochs=1000):\n",
    "    #initialize weights\n",
    "    w1 = np.random.randn(X.shape[1]+1, 3)\n",
    "    w2 = np.random.randn(3+1,3)\n",
    "    w3 = np.random.randn(3+1, 2) #+1 for bias\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        o, h1, h2, Xc, h1c, h2c = forward(X,w1,w2,w3) #output, h1, h2, Xc, h1c, h2c\n",
    "        dW3, dW2, dW1 = backward(o,y,w2,w3,h2,h1,h1c,h2c,Xc) #outputs gradient of weights\n",
    "        w1, w2, w3 = sgd([w1,w2,w3],[dW1,dW2,dW3],eta) #update weights using sgd\n",
    "        if epoch % 100 == 0:\n",
    "           print(f\"Epoch {epoch}: Loss = {np.mean(np.square(o - y))}\")\n",
    "    return o, w1, w2, w3 #weights need to be returned to be used in the future\n",
    "\n",
    "print(train(X,y,0.0001,10000)[0]) #the code error is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
